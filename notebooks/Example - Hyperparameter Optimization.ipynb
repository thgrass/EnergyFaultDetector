{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Example: Hyperparameter optimization with Optuna\n",
    "\n",
    "This notebook introduces 3 examples fÃ¼r hyperparameter optimization based on different optimization objectives.\n",
    "1. Optimizing the Autoencoder reconstruction using the MSE\n",
    "2. Optimizing the FaultDetector classification performance using the Fbeta score\n",
    "3. Optimizing the FaultDetector classification performance using the CARE-score\n",
    "\n",
    "The optimization is done using the [CARE to Compare dataset](https://doi.org/10.5281/zenodo.14958989)"
   ],
   "id": "acc177b6ece47b21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import optuna as op\n",
    "import numpy as np\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "from energy_fault_detector import FaultDetector, Config\n",
    "from energy_fault_detector.evaluation import CAREScore, Care2CompareDataset"
   ],
   "id": "f498cdd89c2cf406"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data_path = './Care_To_Compare'",
   "id": "641b5084847234b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Optimize autoencoder reconstruction",
   "id": "d5ac0225fa5e566b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Our test set (a specific event from the CARE2Compare dataset)\n",
    "c2c = Care2CompareDataset(data_path)\n",
    "event_id = 47\n",
    "train_data, normal_index, _, _ = c2c.load_and_format_event_dataset(event_id=event_id, index_column='time_stamp')\n",
    "\n",
    "# Model configuration starting point\n",
    "model_config = Config('c2c_configs/windfarm_C.yaml')\n",
    "c2c.update_c2c_config(model_config, 'C')\n",
    "\n",
    "# speed up for testing (select a small part of the dataset)\n",
    "N = 10000\n",
    "normal_index = normal_index.iloc[:N]\n",
    "train_data = train_data.iloc[:N]\n",
    "\n",
    "# Create an objective - what should be optimized? --> MSE of the reconstruction error\n",
    "# NOTE:\n",
    "# you can increase the speed of this part slightly if you do not need to fit the datapreprocessor.\n",
    "# in that case, you would fit and apply the data preprocessor outside of this function and only fit the autoencoder inside the objective.\n",
    "def reconstruction_mse(trial: op.Trial) -> float:\n",
    "    \"\"\"Samples new hyperparameters. fits a new model and returns the reconstruction error (MSE) of the validation data.\n",
    "\n",
    "    Args:\n",
    "        trial: optuna Trial object\n",
    "\n",
    "    Returns:\n",
    "        MSE of the reconstruction.\n",
    "    \"\"\"\n",
    "    # Use a fresh config dict per trial\n",
    "    cfg = deepcopy(model_config.config_dict)\n",
    "\n",
    "    autoencoder_params = cfg['train']['autoencoder']['params']\n",
    "\n",
    "    # sample new parameters\n",
    "    autoencoder_params['batch_size'] = int(trial.suggest_categorical(name='batch_size', choices=[32, 64, 128]))\n",
    "    autoencoder_params['learning_rate'] = trial.suggest_float(name='learning_rate', low=1e-5, high=0.01, log=True)\n",
    "    autoencoder_params['decay_rate'] = trial.suggest_float(name='decay_rate', low=0.8, high=0.99)\n",
    "\n",
    "    # architecture\n",
    "    autoencoder_params['layers'][0] = trial.suggest_int(name='layers_0', low=100, high=400)\n",
    "    autoencoder_params['layers'][1] = trial.suggest_int(name='layers_1', low=50, high=100)\n",
    "    autoencoder_params['code_size'] = trial.suggest_int(name='code_size', low=10, high=30)\n",
    "\n",
    "    # create a new model using our new configuration and train the model\n",
    "    model = FaultDetector(Config(config_dict=cfg))\n",
    "    # For autoencoder optimization, we do not need to fit a threshold\n",
    "    training_result = model.fit(train_data, normal_index=normal_index, fit_autoencoder_only=True, save_model=False)\n",
    "\n",
    "    # Calculate the MSE of the reconstruction errors of the validation data - this is minimized\n",
    "    deviations = training_result.val_recon_error\n",
    "    score = np.mean((np.square(deviations)))\n",
    "\n",
    "    return score"
   ],
   "id": "6cc9b7bee0de0a25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "study = op.create_study(sampler=op.samplers.TPESampler(), study_name='autoencoder_optimization', direction='minimize')\n",
    "\n",
    "# if we want to ensure that the first trial is done with the hyperparameters of the configuration, we need to enqueue a trial:\n",
    "autoencoder_params = model_config.config_dict['train']['autoencoder']['params']\n",
    "study.enqueue_trial(params={\n",
    "    'batch_size': autoencoder_params['batch_size'],\n",
    "    'learning_rate': autoencoder_params['learning_rate'],\n",
    "    'layers_0': autoencoder_params['layers'][0],\n",
    "    'layers_1': autoencoder_params['layers'][1],\n",
    "    'code_size': autoencoder_params['code_size'],\n",
    "})\n",
    "\n",
    "# Run optimization for 5 trials\n",
    "study.optimize(reconstruction_mse, n_trials=5)"
   ],
   "id": "1036f11c1b97e12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "study.best_params",
   "id": "7d452e30dd1a1e0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# analyze results\n",
    "study.trials_dataframe()"
   ],
   "id": "5f061454dc9f7980"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Optimize fault detection model - F-beta score",
   "id": "80664d61f648132f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Our test set (a specific event from the CARE2Compare dataset)\n",
    "c2c = Care2CompareDataset(data_path)\n",
    "event_id = 47\n",
    "train_data, normal_index, test_data, test_normal_index = c2c.load_and_format_event_dataset(event_id=event_id, index_column='time_stamp')\n",
    "\n",
    "# Create a ground truth for this event\n",
    "event_info = c2c.event_info_all[c2c.event_info_all['event_id'] == event_id].iloc[0]\n",
    "ground_truth = CAREScore.create_ground_truth(\n",
    "    event_label=event_info['event_label'],\n",
    "    event_start=event_info['event_start'],\n",
    "    event_end=event_info['event_end'],\n",
    "    normal_index=test_normal_index\n",
    ")"
   ],
   "id": "3d4b96de7af51a2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Model configuration starting point\n",
    "model_config = Config('c2c_configs/windfarm_C.yaml')\n",
    "c2c.update_c2c_config(model_config, 'C')\n",
    "\n",
    "# speed up for testing (select a small part of the dataset)\n",
    "N = 10000\n",
    "normal_index = normal_index.iloc[:N]\n",
    "train_data = train_data.iloc[:N]\n",
    "\n",
    "# helper function to (re)set the scaling step of the DataPreprocessor\n",
    "def set_scaler_step(cfg: dict, choice: str) -> dict:\n",
    "    \"\"\"Update cfg to use the chosen scaler.\"\"\"\n",
    "    dp = cfg['train'].setdefault('data_preprocessor', {})\n",
    "    steps = dp.get('steps')\n",
    "\n",
    "    # Remove any existing scaler step(s)\n",
    "    scaler_names = {'standard_scaler', 'minmax_scaler'}\n",
    "    steps = [s for s in steps if s.get('name') not in scaler_names]\n",
    "    # Add the chosen scaler step\n",
    "    if choice == 'minmax':\n",
    "        steps.append({'name': 'minmax_scaler'})\n",
    "    else:\n",
    "        # 'standardize'\n",
    "        steps.append({'name': 'standard_scaler'})\n",
    "\n",
    "    dp['steps'] = steps\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def f_score(trial: op.Trial) -> float:\n",
    "    \"\"\"Returns the F-score of the model (only useful for datasets with anomalies).\n",
    "\n",
    "    Args:\n",
    "        trial: optuna Trial object\n",
    "\n",
    "    Returns:\n",
    "        Score of the FaultDetector model \n",
    "    \"\"\"\n",
    "    # Use a fresh config dict per trial\n",
    "    cfg = deepcopy(model_config.config_dict)\n",
    "\n",
    "    # Scale choice (new steps mode or legacy fallback)\n",
    "    scale_choice = trial.suggest_categorical('scale', ['minmax', 'standardize'])\n",
    "    cfg = set_scaler_step(cfg, scale_choice)\n",
    "\n",
    "    # Autoencoder params\n",
    "    autoencoder_params = cfg['train']['autoencoder']['params']\n",
    "    autoencoder_params['batch_size'] = int(trial.suggest_categorical(name='batch_size', choices=[32, 64, 128]))\n",
    "    autoencoder_params['learning_rate'] = trial.suggest_float(name='learning_rate', low=1e-5, high=0.01, log=True)\n",
    "    autoencoder_params['decay_rate'] = trial.suggest_float(name='decay_rate', low=0.8, high=0.99)\n",
    "\n",
    "    # architecture\n",
    "    autoencoder_params['layers'][0] = trial.suggest_int(name='layers_0', low=100, high=400)\n",
    "    autoencoder_params['layers'][1] = trial.suggest_int(name='layers_1', low=50, high=100)\n",
    "    autoencoder_params['code_size'] = trial.suggest_int(name='code_size', low=10, high=30)\n",
    "\n",
    "    # create a new model using our new configuration and train the model\n",
    "    model = FaultDetector(Config(config_dict=cfg))\n",
    "    _ = model.fit(train_data, normal_index=normal_index, save_models=False)\n",
    "    predictions = model.predict(test_data)\n",
    "\n",
    "    return fbeta_score(\n",
    "        y_true=ground_truth.sort_index(),\n",
    "        y_pred=predictions.predicted_anomalies.sort_index(),\n",
    "        beta=0.5\n",
    "    )"
   ],
   "id": "2d93c80b0090b855"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "study = op.create_study(sampler=op.samplers.TPESampler(), study_name='ad_optimization', direction='maximize')\n",
    "\n",
    "# if we want to ensure that the first trial is done with the hyperparameters of the configuration, we need to enqueue a trial:\n",
    "autoencoder_params = model_config.config_dict['train']['autoencoder']['params']\n",
    "study.enqueue_trial(params={\n",
    "    'batch_size': autoencoder_params['batch_size'],\n",
    "    'learning_rate': autoencoder_params['learning_rate'],\n",
    "    'layers_0': autoencoder_params['layers'][0],\n",
    "    'layers_1': autoencoder_params['layers'][1],\n",
    "    'code_size': autoencoder_params['code_size'],\n",
    "})\n",
    "\n",
    "study.optimize(f_score, n_trials=5)"
   ],
   "id": "d8a1cd60b4664efd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "study.trials_dataframe()",
   "id": "49d60813c32812ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Optimize fault detection model - CARE score\n",
    "Optimize the CARE Score. Note that this takes a while, as we train a model for each subdataset."
   ],
   "id": "bb8fc58e17ba1b64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Our test set - Wind Farm B from the CARE2Compare dataset\n",
    "c2c = Care2CompareDataset(data_path)\n",
    "wind_farm = 'B'\n",
    "\n",
    "# Model configuration starting point\n",
    "model_config = Config('c2c_configs/windfarm_B.yaml')\n",
    "c2c.update_c2c_config(model_config, 'B')\n",
    "\n",
    "# speed up for testing\n",
    "N = 10000\n",
    "max_datasets = 15\n",
    "\n",
    "def care_objective(trial: op.Trial) -> float:\n",
    "    \"\"\"Returns the CARE score of the FaultDetector model.\"\"\"\n",
    "\n",
    "    # Use a fresh config dict per trial\n",
    "    cfg = deepcopy(model_config.config_dict)\n",
    "\n",
    "    autoencoder_params = cfg['train']['autoencoder']['params']\n",
    "    threshold_params = cfg['train']['threshold_selector']['params']\n",
    "\n",
    "    autoencoder_params['batch_size'] = int(trial.suggest_categorical(name='batch_size', choices=[32, 64, 128]))\n",
    "    autoencoder_params['learning_rate'] = trial.suggest_float(name='learning_rate', low=1e-5, high=0.01, log=True)\n",
    "\n",
    "    # architecture\n",
    "    autoencoder_params['layers'][0] = trial.suggest_int(name='layers_0', low=20, high=100)\n",
    "    autoencoder_params['code_size'] = trial.suggest_int(name='code_size', low=5, high=20)\n",
    "\n",
    "    # threshold\n",
    "    threshold_params['gamma'] = trial.suggest_float(name='gamma', low=0.05, high=0.3)\n",
    "    threshold_params['nn_size'] = trial.suggest_int(name='nn_size', low=20, high=50)\n",
    "\n",
    "    # Create a CAREScore object and train+evaluate each dataset for this wind farm\n",
    "    care_score = CAREScore(coverage_beta=0.5, eventwise_f_score_beta=0.5, anomaly_detection_method='criticality')\n",
    "    i = 1\n",
    "    for x_train, y_train, x_test, y_test, event_id in c2c.iter_formatted_datasets(wind_farm=wind_farm, index_column='time_stamp'):\n",
    "        print(f\"event {i}/{len(c2c.event_info_all[c2c.event_info_all['wind_farm'] == wind_farm])}\")\n",
    "        if N is not None:\n",
    "            x_train = x_train.iloc[:N]\n",
    "            x_test = x_test.iloc[:N]\n",
    "            y_train = y_train.iloc[:N]\n",
    "            y_test = y_test.iloc[:N]\n",
    "        \n",
    "        # create a new model using our new configuration and train the model\n",
    "        model = FaultDetector(Config(config_dict=cfg))\n",
    "        _ = model.fit(x_train, normal_index=y_train, save_models=False)\n",
    "        prediction = model.predict(x_test)\n",
    "        event_info = c2c.event_info_all[c2c.event_info_all['event_id'] == event_id].iloc[0]\n",
    "        care_score.evaluate_event(\n",
    "            event_id=event_id,\n",
    "            event_start=event_info['event_start'],\n",
    "            event_end=event_info['event_end'],\n",
    "            event_label=event_info['event_label'],\n",
    "            normal_index=y_test,\n",
    "            predicted_anomalies=prediction.predicted_anomalies,\n",
    "            ignore_normal_index=False\n",
    "        )\n",
    "        i += 1\n",
    "        if i > max_datasets:\n",
    "            break\n",
    "\n",
    "    score = care_score.get_final_score()\n",
    "\n",
    "    return score"
   ],
   "id": "ce3000b2686a463f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "study = op.create_study(sampler=op.samplers.TPESampler(), study_name='care_optimization', direction='maximize')\n",
    "\n",
    "# Ensure that the first trial is done with the hyperparameters of the provided configuration\n",
    "autoencoder_params = model_config.config_dict['train']['autoencoder']['params']\n",
    "threshold_params = model_config.config_dict['train']['threshold_selector']['params']\n",
    "study.enqueue_trial(params={\n",
    "    'batch_size': autoencoder_params['batch_size'],\n",
    "    'learning_rate': autoencoder_params['learning_rate'],\n",
    "    'layers_0': autoencoder_params['layers'][0],\n",
    "    'code_size': autoencoder_params['code_size'],\n",
    "    'gamma': threshold_params['gamma'],\n",
    "    'nn_size': threshold_params['nn_size'],\n",
    "})\n",
    "\n",
    "# Since we loop through many datasets, train many models, we run the garbage collector after each trial\n",
    "study.optimize(care_objective, n_trials=5, gc_after_trial=True)"
   ],
   "id": "2063c7f60b979cc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "study.trials_dataframe()",
   "id": "95c8498e85467561"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
